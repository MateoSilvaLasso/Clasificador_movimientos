{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos(directorio):\n",
    "    data = []\n",
    "    etiquetas = []\n",
    "    for carpeta in os.listdir(directorio):\n",
    "        subdirectorio = os.path.join(directorio, carpeta, 'csv')\n",
    "        \n",
    "        for archivo in glob.glob(os.path.join(subdirectorio, '*.csv')):\n",
    "            df = pd.read_csv(archivo)\n",
    "            data.append(df)\n",
    "            etiquetas.append(carpeta)\n",
    "    return data, etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of Coordinates\n",
    "\n",
    "At this stage, we take the hip landmarks and normalize them by calculating their average. This is done to ensure that different body types do not result in significantly different predictions regarding the activity being performed.\n",
    "\n",
    "Additionally, we apply the same process to the upper body, using the position of the shoulders for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_coordenadas(df):\n",
    "    #caderas\n",
    "    cadera_derecha = df[df['landmark_index'] == 23][['x', 'y', 'z']].mean()\n",
    "    cadera_izquierda = df[df['landmark_index'] == 24][['x', 'y', 'z']].mean()\n",
    "    cadera_centro = (cadera_derecha + cadera_izquierda) / 2\n",
    "    \n",
    "   \n",
    "    df['x'] -= cadera_centro['x']\n",
    "    df['y'] -= cadera_centro['y']\n",
    "    df['z'] -= cadera_centro['z']\n",
    "    \n",
    "   #hombros\n",
    "    hombro_derecho = df[df['landmark_index'] == 11][['x', 'y', 'z']].mean()\n",
    "    hombro_izquierdo = df[df['landmark_index'] == 12][['x', 'y', 'z']].mean()\n",
    "    torso_tamano = np.linalg.norm(hombro_derecho - hombro_izquierdo)\n",
    "    \n",
    "    df['x'] /= torso_tamano\n",
    "    df['y'] /= torso_tamano\n",
    "    df['z'] /= torso_tamano\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Noise\n",
    "\n",
    "At this stage, our goal is to reduce noise in each position of every landmark in the dataset to achieve better predictions, as excessive noise can negatively impact the prediction accuracy.\n",
    "\n",
    "Using the function ```gaussian_filter1d```, we apply a one-dimensional Gaussian filter to each coordinate. This allows us to smooth the data for each coordinate with a standard deviation of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_datos(df):\n",
    "    df['x'] = gaussian_filter1d(df['x'], sigma=2)\n",
    "    df['y'] = gaussian_filter1d(df['y'], sigma=2)\n",
    "    df['z'] = gaussian_filter1d(df['z'], sigma=2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Angles\n",
    "\n",
    "At this stage, this function takes three points as parameters, calculates the vectors, and computes the cosine of the angle to ultimately determine the angle of these postures in radians.\n",
    "\n",
    "This step is crucial, as when classifying movements, this function makes it easier to understand the type of trajectory a person is performing based on the position of their body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_angulo(p1, p2, p3):\n",
    "    vector1 = p1 - p2\n",
    "    vector2 = p3 - p2\n",
    "    cos_theta = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    angulo = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "    return np.degrees(angulo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Features for Training\n",
    "\n",
    "Finally, in this data filtering process, we generate all the necessary features for the model. Using the corresponding landmarks, we extract the required features to ensure accurate and effective training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de características agregadas\n",
    "def generar_caracteristicas(df):\n",
    "    # Articulaciones clave: hombros, codos, y caderas\n",
    "    articulaciones_clave = [11, 12, 13, 14, 15, 16, 23, 24]\n",
    "    \n",
    "    velocidades = []\n",
    "    angulos_codo_derecho = []\n",
    "    angulos_codo_izquierdo = []\n",
    "    angulos_tronco = []\n",
    "    \n",
    "    for i in range(1, len(df)):\n",
    "        fila_actual = df[df['frame'] == i]\n",
    "        fila_anterior = df[df['frame'] == i - 1]\n",
    "        \n",
    "        # Calcular velocidades\n",
    "        for articulacion in articulaciones_clave:\n",
    "            actual = fila_actual[fila_actual['landmark_index'] == articulacion]\n",
    "            anterior = fila_anterior[fila_anterior['landmark_index'] == articulacion]\n",
    "            \n",
    "            if not actual.empty and not anterior.empty:\n",
    "                vel_x = actual['x'].values[0] - anterior['x'].values[0]\n",
    "                vel_y = actual['y'].values[0] - anterior['y'].values[0]\n",
    "                vel_z = actual['z'].values[0] - anterior['z'].values[0]\n",
    "                velocidad = np.sqrt(vel_x**2 + vel_y**2 + vel_z**2)\n",
    "                velocidades.append(velocidad)\n",
    "\n",
    "        # Calcular ángulos\n",
    "        hombro_derecho = fila_actual[fila_actual['landmark_index'] == 11][['x', 'y', 'z']].values\n",
    "        codo_derecho = fila_actual[fila_actual['landmark_index'] == 13][['x', 'y', 'z']].values\n",
    "        muneca_derecha = fila_actual[fila_actual['landmark_index'] == 15][['x', 'y', 'z']].values\n",
    "        \n",
    "        if hombro_derecho.size > 0 and codo_derecho.size > 0 and muneca_derecha.size > 0:\n",
    "            angulo_codo_derecho = calcular_angulo(hombro_derecho[0], codo_derecho[0], muneca_derecha[0])\n",
    "            angulos_codo_derecho.append(angulo_codo_derecho)\n",
    "        \n",
    "        hombro_izquierdo = fila_actual[fila_actual['landmark_index'] == 12][['x', 'y', 'z']].values\n",
    "        codo_izquierdo = fila_actual[fila_actual['landmark_index'] == 14][['x', 'y', 'z']].values\n",
    "        muneca_izquierda = fila_actual[fila_actual['landmark_index'] == 16][['x', 'y', 'z']].values\n",
    "        \n",
    "        if hombro_izquierdo.size > 0 and codo_izquierdo.size > 0 and muneca_izquierda.size > 0:\n",
    "            angulo_codo_izquierdo = calcular_angulo(hombro_izquierdo[0], codo_izquierdo[0], muneca_izquierda[0])\n",
    "            angulos_codo_izquierdo.append(angulo_codo_izquierdo)\n",
    "        \n",
    "        cadera_centro = ((fila_actual[fila_actual['landmark_index'] == 23][['x', 'y', 'z']].values +\n",
    "                          fila_actual[fila_actual['landmark_index'] == 24][['x', 'y', 'z']].values) / 2)\n",
    "        \n",
    "        if hombro_derecho.size > 0 and cadera_centro.size > 0 and hombro_izquierdo.size > 0:\n",
    "            angulo_tronco = calcular_angulo(hombro_derecho[0], cadera_centro[0], hombro_izquierdo[0])\n",
    "            angulos_tronco.append(angulo_tronco)\n",
    "    \n",
    "    # Calcular estadísticas de cada característica\n",
    "    caracteristicas = [\n",
    "        np.mean(velocidades), np.std(velocidades),\n",
    "        np.mean(angulos_codo_derecho), np.std(angulos_codo_derecho),\n",
    "        np.mean(angulos_codo_izquierdo), np.std(angulos_codo_izquierdo),\n",
    "        np.mean(angulos_tronco), np.std(angulos_tronco)\n",
    "    ]\n",
    "    \n",
    "    return caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_y_extraer(data):\n",
    "    datos_procesados = []\n",
    "    for df in data:\n",
    "        df = normalizar_coordenadas(df)\n",
    "        df = filtrar_datos(df)\n",
    "        caracteristicas = generar_caracteristicas(df)\n",
    "        datos_procesados.append(caracteristicas)\n",
    "    return datos_procesados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate bar charts to compare the metrics of the models.\n",
    "\n",
    "    Parameters:\n",
    "        nombres (list): List of models names.\n",
    "        accuracy_list (list): List of Accuracy values for the models.\n",
    "        f1_list (list):  List of F1-score values for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_graficos(nombres, accuracy_list, f1_list):\n",
    "    x = range(len(nombres))  # Posiciones para las barras\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    \n",
    "    plt.bar(x, accuracy_list, width=0.4, label='Accuracy', alpha=0.7, color='blue', align='center')\n",
    "    \n",
    "    \n",
    "    plt.bar(x, f1_list, width=0.4, label='F1-score', alpha=0.7, color='green', align='edge')\n",
    "\n",
    "    \n",
    "    plt.xlabel('Modelos')\n",
    "    plt.ylabel('Métrica')\n",
    "    plt.title('Comparación de Accuracy y F1-score entre modelos')\n",
    "    plt.xticks(x, nombres, rotation=45, ha='right')  \n",
    "    plt.legend()  \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Mostrar el gráfico\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "En el proceso de entrenamiento tomamos en esta caso tres modelos los cuales son random forest, xgboost y SVM para hacer clasificacion y encontrar el mejor modelo, esto lo hacemos con GridSearch para encontrar el mejor modelo posible configurando los parametros para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "def entrenar_modelo_random_forest(datos_procesados, etiquetas):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        datos_procesados, etiquetas, test_size=0.2, random_state=42,\n",
    "    )\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 250],\n",
    "        'max_depth': [100, 200, 300],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42, class_weight='balanced'), param_grid=param_grid, cv=skf, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(grid_search.best_params_)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo_xgboost(datos_procesados, etiquetas):\n",
    "    label_encoder = LabelEncoder()\n",
    "    etiquetas_numericas = label_encoder.fit_transform(etiquetas)\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        datos_procesados, etiquetas_numericas, test_size=0.2, random_state=42,\n",
    "    )\n",
    "\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],  \n",
    "        'max_depth': [3, 5, 10],  \n",
    "        'learning_rate': [0.01, 0.1, 0.2], \n",
    "        'subsample': [0.6, 0.8, 1.0],  \n",
    "        'colsample_bytree': [0.6, 0.8, 1.0], \n",
    "        'gamma': [0, 1, 5],  \n",
    "        'scale_pos_weight': [1, 2, 5]  \n",
    "    }\n",
    "\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb.XGBClassifier(\n",
    "            objective='multi:softmax',  \n",
    "            num_class=len(set(etiquetas_numericas)),  \n",
    "            random_state=42,\n",
    "            use_label_encoder=False  \n",
    "        ),\n",
    "        param_grid=param_grid,\n",
    "        cv=skf,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "   \n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "   \n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    \n",
    "    y_test_text = label_encoder.inverse_transform(y_test)\n",
    "    y_pred_text = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "    print(classification_report(y_test_text, y_pred_text))\n",
    "    print(\"Mejores hiperparámetros encontrados:\", grid_search.best_params_)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo_svm(datos_procesados, etiquetas):\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        datos_procesados, etiquetas, test_size=0.25, random_state=42\n",
    "    )\n",
    "    \n",
    "    \n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],  \n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  \n",
    "        'gamma': ['scale', 'auto'],  \n",
    "        'degree': [2, 3, 4],  \n",
    "    }\n",
    "    \n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=SVC(class_weight='balanced', probability=True, random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        cv=skf,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "def guardar_modelo(modelo, nombre_archivo):\n",
    "    joblib.dump(modelo, nombre_archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio_datos = '../data/processed'\n",
    "data, etiquetas = cargar_datos(directorio_datos)\n",
    "datos_procesados = preprocesar_y_extraer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             Sentarse       0.50      0.33      0.40         3\n",
      "  caminar hacia atras       0.83      0.83      0.83         6\n",
      "caminar hacia delante       0.50      1.00      0.67         3\n",
      "                girar       1.00      0.80      0.89         5\n",
      "              pararse       0.50      0.40      0.44         5\n",
      "\n",
      "             accuracy                           0.68        22\n",
      "            macro avg       0.67      0.67      0.65        22\n",
      "         weighted avg       0.70      0.68      0.68        22\n",
      "\n",
      "{'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "modelo_random_forest = entrenar_modelo_random_forest(datos_procesados, etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2187 candidates, totalling 10935 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mateo\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:08:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             Sentarse       1.00      0.33      0.50         3\n",
      "  caminar hacia atras       0.86      1.00      0.92         6\n",
      "caminar hacia delante       0.50      1.00      0.67         3\n",
      "                girar       1.00      0.80      0.89         5\n",
      "              pararse       0.50      0.40      0.44         5\n",
      "\n",
      "             accuracy                           0.73        22\n",
      "            macro avg       0.77      0.71      0.68        22\n",
      "         weighted avg       0.78      0.73      0.71        22\n",
      "\n",
      "Mejores hiperparámetros encontrados: {'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'scale_pos_weight': 1, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "modelo_xgboost = entrenar_modelo_xgboost(datos_procesados, etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             Sentarse       1.00      0.33      0.50         3\n",
      "  caminar hacia atras       0.86      1.00      0.92         6\n",
      "caminar hacia delante       0.86      1.00      0.92         6\n",
      "                girar       1.00      0.86      0.92         7\n",
      "              pararse       0.83      1.00      0.91         5\n",
      "\n",
      "             accuracy                           0.89        27\n",
      "            macro avg       0.91      0.84      0.84        27\n",
      "         weighted avg       0.91      0.89      0.87        27\n",
      "\n",
      "Mejores hiperparámetros: {'C': 0.1, 'degree': 3, 'gamma': 'auto', 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "modelo_svm = entrenar_modelo_svm(datos_procesados, etiquetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance metrics of various models and save the best one.\n",
    "\n",
    "Parameters:  \n",
    "- `modelos` (list): List of trained models.  \n",
    "- `nombres` (list): List of names corresponding to the models.  \n",
    "- `X_test` (ndarray): Features of the test set.  \n",
    "- `y_test` (ndarray): Labels of the test set.  \n",
    "- `path_guardar` (str): Path where the best model will be saved.\n",
    "\n",
    "The goal at this stage is to compare all models and select the one with the best F1-score. This ensures the chosen model is the most balanced in terms of true positives and true negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_y_guardar_modelo(modelos, nombres, X_test, y_test, path_guardar):\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_test_encoded = label_encoder.fit_transform(y_test)  \n",
    "    \n",
    "    mejores_métricas = {'modelo': None, 'nombre': '', 'f1_score': 0}\n",
    "\n",
    "    accuracy_list = []\n",
    "    f1_list = []\n",
    "    \n",
    "    for modelo, nombre in zip(modelos, nombres):\n",
    "        \n",
    "        y_pred = modelo.predict(X_test)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            y_pred_encoded = label_encoder.transform(y_pred)  \n",
    "        except ValueError:\n",
    "            y_pred_encoded = y_pred  \n",
    "        \n",
    "        \n",
    "        accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
    "        f1 = f1_score(y_test_encoded, y_pred_encoded, average='weighted') \n",
    "\n",
    "        \n",
    "        accuracy_list.append(accuracy)\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "        \n",
    "        print(f\"Métricas para {nombre}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}\\n\")\n",
    "        \n",
    "        \n",
    "        if f1 > mejores_métricas['f1_score']:\n",
    "            mejores_métricas['modelo'] = modelo\n",
    "            mejores_métricas['nombre'] = nombre\n",
    "            mejores_métricas['f1_score'] = f1\n",
    "    \n",
    "    \n",
    "    print(f\"Mejor modelo: {mejores_métricas['nombre']} con F1-score: {mejores_métricas['f1_score']:.4f}\")\n",
    "    guardar_modelo(mejores_métricas['modelo'], path_guardar)\n",
    "    print(f\"Modelo guardado en {path_guardar}\")\n",
    "\n",
    "\n",
    "    generar_graficos(nombres, accuracy_list, f1_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, at this point, we can conclude that the best model is the SVM (built with SVC, which is SVM for classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelo_random_forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m modelos \u001b[38;5;241m=\u001b[39m [\u001b[43mmodelo_random_forest\u001b[49m, modelo_xgboost, modelo_svm]\n\u001b[0;32m      2\u001b[0m nombres \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVM\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m      6\u001b[0m     datos_procesados, etiquetas, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'modelo_random_forest' is not defined"
     ]
    }
   ],
   "source": [
    "modelos = [modelo_random_forest, modelo_xgboost, modelo_svm]\n",
    "nombres = ['Random Forest', 'XGBoost', 'SVM']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    datos_procesados, etiquetas, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "comparar_y_guardar_modelo(modelos, nombres, X_test, y_test, '../models/modelo_clasificacion_actividades.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
